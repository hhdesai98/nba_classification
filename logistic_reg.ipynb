{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_train = pd.read_csv('Data_Scripting_Cleaning/Full_data/Training_Sets/nba_train.csv')\n",
    "nba_test = pd.read_csv('Data_Scripting_Cleaning/Full_data/Test_Sets/nba_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be fitting a logistic regression model with an L1 penalty to encourage coefficient sparsity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can describe the model of interest under a statistical framework. Denote the following quantities:\n",
    "1) y: an n x 1 vector, containing the binary variable of interest\n",
    "2) X: and n x (p+1) matrix, consisting of the feature variables and intercept\n",
    "3) $\\beta$: A (p+1) x 1 vector of coefficients.\n",
    "\n",
    "We will also use the following functions:\n",
    "\n",
    "1) $logit(p) = log(\\frac{p}{1-p})$; this is often denoted as the log-odds\n",
    "2) $expit(x) = \\frac{1}{1+exp(-x)}$; this is the inverse function of logit, i.e. $expit(x) = logit^{-1}(x)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will be utilizing is:\n",
    "$$\n",
    "y_i \\sim Bernoulli(p_i = expit(x_i^{T}\\beta))\n",
    "$$\n",
    "\n",
    "Where $x_i$ is the i'th row of the feature matrix X."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume independence (clearly broken here since player performance is clearly correlated across different seasons, but we will disregard this for now), then we have a likelihood function of the form:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^n (expit(x_i^{T}\\beta))^{y_i}\\times (1-expit(x_i^{T}\\beta))^{1-y_i}\n",
    "$$\n",
    "\n",
    "Leading to a log-likelihood function (our unregularized negative objective function) of:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_{i=1}^n y_i(log(expit(x_i^{T}\\beta))) + (1-y_i)log(1-expit(x_i^{T}\\beta)) \\\\ = \\sum_{i=1}^n y_i(x_i^T\\beta)-log(1+exp(x_i^T\\beta))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we will be finding:\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{min}\\sum_{i=1}^n -y_i(x_i^T\\beta)+log(1+exp(x_i^T\\beta)) + r(\\beta)\n",
    "$$\n",
    "\n",
    "where $r(\\beta)$ is a regularization term"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the L1 regularizer SKlearn specifically will be minimizing:\n",
    "$$\n",
    "\\underset{\\beta}{min} \\ C\\sum_{i=1}^n -y_i(x_i^T\\beta)+log(1+exp(x_i^T\\beta)) + \\sum_{i=0}^p|\\beta_i|\n",
    "$$\n",
    "\n",
    "Where the regularizing constant is given as C>0. This is a hyperparameter we must tune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Logistic Model to NBA Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin to apply this model to our data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can examine the proportions of All-NBA in our dataset. We initially created a train-test split where we attempt to create similar distributions for each set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.967255\n",
       "1    0.032745\n",
       "Name: all_nba_c_year, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Proportions of all_nba_c_year\n",
    "nba_train['all_nba_c_year'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the testing stat we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.96727\n",
       "1    0.03273\n",
       "Name: all_nba_c_year, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_test['all_nba_c_year'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sets have similar proportions as expected, but clearly we have an incredibly unbalanced dataset, but we may try to fit our models initially to see if we can improve upon them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we may try to filter our data based on variables like minutes played (`MP`), games played (`G`), since we know that these awards go to the best players in the NBA, and the best players tend to play a lot. In the new 2023 CBA (collective bargaining agreement), there is a minimum game requirement (65 games) that must be met in order to win All-NBA. However, since this rule was not in place for prior awards (where this data comes from), we can instead filter so that we only consider players who have played more games than the players with the least minutes played and games played that still won All-NBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_minutes = nba_train[(nba_train['all_nba_c_year']==1)].MP.min()\n",
    "min_G = nba_train[(nba_train['all_nba_c_year']==1)].G.min()\n",
    "nba_filt_train = nba_train[(nba_train['MP']>=min_minutes) & (nba_train['G']>=min_G)]\n",
    "nba_filt_test = nba_test[(nba_test['MP']>=min_minutes) & (nba_test['G']>=min_G)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train = nba_filt_train['all_nba_c_year']\n",
    "\n",
    "y_test = nba_filt_test['all_nba_c_year']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we may fit a simpler model. Which players will make all-nba, versus which players won't? In this case we will ignore teams and instead only focus on the binary indicator. We will also see how the classifier does when we don't filter for positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct this model using a current year players stats and predict whether they make all_nba in the current year."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have already standardized our variables, we must simply one hot encode the categorical variable, `Tm`. We will use a pipeline for ease of use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the following parameter grid for C. We will select the best C based off of k-fold cross validation, with k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "num_features = ['Age','G', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%',\n",
    "       '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB',\n",
    "       'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'PER', 'TS%', '3PAr', 'FTr',\n",
    "       'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS',\n",
    "       'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP', 'W',\n",
    "       'num_all_nba']\n",
    "\n",
    "cat_features = ['Tm']\n",
    "\n",
    "#Now I will create a pipeline where I extract my_features, and apply OHE to cat_features\n",
    "ct = ColumnTransformer(\n",
    "    [(\"select\", \"passthrough\", num_features),\n",
    "     (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)],\n",
    "     remainder=\"drop\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"col_transform\", ct),\n",
    "    (\"classifier\", LogisticRegression(penalty = 'l2', solver = 'liblinear', \n",
    "                                      max_iter = 10000, random_state=0))\n",
    "])\n",
    "\n",
    "#consider grid of C values between 0 and 1. Larger interval was \n",
    "#checked first, and then the interval was narrowed down to 0 to 1\n",
    "param_grid2 = {'classifier__C': np.arange(0.01, 1.01, 0.01)}\n",
    "model = GridSearchCV(clf, param_grid2)\n",
    "\n",
    "model.fit(nba_filt_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the following metrics for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       819\n",
      "           1       0.81      0.74      0.77       123\n",
      "\n",
      "    accuracy                           0.94       942\n",
      "   macro avg       0.89      0.86      0.87       942\n",
      "weighted avg       0.94      0.94      0.94       942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, model.predict(nba_filt_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.15}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('mean_fit_time', array([0.01442361, 0.02235103, 0.02797999, 0.02222881, 0.03539066,\n",
       "       0.03637457, 0.06073017, 0.08284574, 0.07104864, 0.09200077,\n",
       "       0.06417117, 0.10403972, 0.10013118, 0.10272603, 0.13682137,\n",
       "       0.11112041, 0.12752604, 0.10954585, 0.12594862, 0.14090967,\n",
       "       0.14731917, 0.10359397, 0.11989121, 0.13967462, 0.14225078,\n",
       "       0.13397398, 0.13095846, 0.13387766, 0.16000485, 0.15493932,\n",
       "       0.18220887, 0.21690211, 0.20321288, 0.22082925, 0.20778275,\n",
       "       0.23309355, 0.32452822, 0.27278118, 0.25449224, 0.31603451,\n",
       "       0.26720324, 0.31058693, 0.33655229, 0.33177915, 0.29757333,\n",
       "       0.35081921, 0.34025154, 0.34134803, 0.33378978, 0.31803379,\n",
       "       0.35030961, 0.32943087, 0.3618618 , 0.351261  , 0.33538704,\n",
       "       0.31403775, 0.35411634, 0.30192242, 0.31864042, 0.3626307 ,\n",
       "       0.31308365, 0.3162643 , 0.32155418, 0.31866269, 0.33440118,\n",
       "       0.29653172, 0.32236867, 0.32921944, 0.32060242, 0.32307625,\n",
       "       0.3262414 , 0.32931933, 0.33590503, 0.29473219, 0.31131096,\n",
       "       0.33343663, 0.30800605, 0.35718241, 0.29791961, 0.31024137,\n",
       "       0.34025278, 0.32158594, 0.3809164 , 0.34640985, 0.31898932,\n",
       "       0.37548037, 0.31663728, 0.30585885, 0.37485542, 0.34080625,\n",
       "       0.33320322, 0.33734055, 0.34881897, 0.32144966, 0.34242749,\n",
       "       0.33551722, 0.38018427, 0.37222023, 0.36503935, 0.36058731])), ('std_fit_time', array([0.00471127, 0.00563067, 0.01026758, 0.00324578, 0.00929336,\n",
       "       0.01259314, 0.01970188, 0.02590911, 0.02224689, 0.03156743,\n",
       "       0.03034053, 0.02351665, 0.03139475, 0.02513417, 0.01934044,\n",
       "       0.03492838, 0.04832223, 0.05730295, 0.05663715, 0.03525125,\n",
       "       0.05848432, 0.01384557, 0.02595991, 0.02480556, 0.06970127,\n",
       "       0.05634142, 0.02121034, 0.03590762, 0.01850179, 0.03313955,\n",
       "       0.04293443, 0.05464963, 0.05455127, 0.04802724, 0.07551523,\n",
       "       0.07368396, 0.07634095, 0.05409883, 0.03154437, 0.03046154,\n",
       "       0.03467659, 0.07095012, 0.0921984 , 0.05744409, 0.07386923,\n",
       "       0.06962169, 0.04652497, 0.04987979, 0.06371288, 0.04804761,\n",
       "       0.0387534 , 0.06457589, 0.08213646, 0.04361036, 0.04141557,\n",
       "       0.0365376 , 0.02995618, 0.04431129, 0.05304421, 0.03196519,\n",
       "       0.03278399, 0.05137086, 0.02712631, 0.04678403, 0.03233658,\n",
       "       0.02471972, 0.03181275, 0.02473897, 0.04332802, 0.06811176,\n",
       "       0.05809958, 0.04890814, 0.05962288, 0.03763405, 0.06102645,\n",
       "       0.0314065 , 0.05381706, 0.06769256, 0.05745338, 0.05063181,\n",
       "       0.03722765, 0.04745579, 0.0347246 , 0.05205214, 0.05719532,\n",
       "       0.04455327, 0.03637109, 0.03535474, 0.01878623, 0.02969353,\n",
       "       0.0254936 , 0.02517453, 0.06858675, 0.02618188, 0.02807969,\n",
       "       0.00860071, 0.05497764, 0.0528216 , 0.03293341, 0.02946752])), ('mean_score_time', array([0.01402001, 0.00489564, 0.00632176, 0.01801577, 0.0117528 ,\n",
       "       0.00983739, 0.00896516, 0.0197825 , 0.00518498, 0.00668712,\n",
       "       0.01220646, 0.00800982, 0.01915555, 0.00668736, 0.01206656,\n",
       "       0.00513158, 0.00726666, 0.01190758, 0.01494365, 0.0187427 ,\n",
       "       0.00768538, 0.01308289, 0.01422443, 0.01412191, 0.02071786,\n",
       "       0.02135019, 0.01144719, 0.01533132, 0.0100585 , 0.00729771,\n",
       "       0.00755358, 0.01799507, 0.00799861, 0.01151977, 0.01157527,\n",
       "       0.00765028, 0.00855904, 0.02210789, 0.01813893, 0.01753964,\n",
       "       0.01253719, 0.03811917, 0.01198006, 0.0124155 , 0.0355062 ,\n",
       "       0.01278224, 0.01059804, 0.01723142, 0.02050076, 0.0240262 ,\n",
       "       0.01578407, 0.01626959, 0.01670494, 0.01621528, 0.01486883,\n",
       "       0.00941195, 0.02447891, 0.0156827 , 0.00709643, 0.03153801,\n",
       "       0.02707119, 0.01208429, 0.01851754, 0.01036673, 0.01720867,\n",
       "       0.03332229, 0.01453671, 0.01160312, 0.01042957, 0.00855217,\n",
       "       0.01628757, 0.01818056, 0.02207084, 0.02045045, 0.0124032 ,\n",
       "       0.01928382, 0.02073498, 0.03710289, 0.0141088 , 0.01112242,\n",
       "       0.02083793, 0.01761346, 0.00590801, 0.04152784, 0.01261525,\n",
       "       0.01012888, 0.01637425, 0.05220933, 0.01131754, 0.01664677,\n",
       "       0.01466012, 0.00863357, 0.03152986, 0.04699121, 0.01029015,\n",
       "       0.01121697, 0.00982513, 0.00657148, 0.01544018, 0.01568141])), ('std_score_time', array([0.01327142, 0.00306632, 0.00258707, 0.01561672, 0.0051635 ,\n",
       "       0.00991513, 0.00560977, 0.01187268, 0.00271253, 0.00494573,\n",
       "       0.00939535, 0.00415171, 0.01380032, 0.00614473, 0.0026671 ,\n",
       "       0.00324948, 0.00436864, 0.00462766, 0.01103205, 0.01406382,\n",
       "       0.00343776, 0.00604332, 0.01080872, 0.00825576, 0.01535669,\n",
       "       0.01532631, 0.01206389, 0.00707615, 0.00720725, 0.00415973,\n",
       "       0.00433147, 0.00621914, 0.00567531, 0.006126  , 0.00600652,\n",
       "       0.00255881, 0.00959373, 0.00964692, 0.00666942, 0.00739744,\n",
       "       0.00557518, 0.05148571, 0.00956593, 0.00415805, 0.04792631,\n",
       "       0.00520474, 0.00554091, 0.00810607, 0.02056952, 0.01487651,\n",
       "       0.00640083, 0.0060265 , 0.00998505, 0.0081402 , 0.00750876,\n",
       "       0.0045526 , 0.02229907, 0.00390914, 0.00396035, 0.04600501,\n",
       "       0.01559946, 0.0077292 , 0.01043029, 0.00521568, 0.012229  ,\n",
       "       0.03517231, 0.00955123, 0.0063819 , 0.00717982, 0.00561976,\n",
       "       0.00434834, 0.01329247, 0.01458393, 0.01319402, 0.00554721,\n",
       "       0.00526197, 0.00745874, 0.04580802, 0.00583911, 0.00546528,\n",
       "       0.01416405, 0.0060402 , 0.00207289, 0.05678539, 0.00420399,\n",
       "       0.00543041, 0.0144071 , 0.0533039 , 0.00735187, 0.00714746,\n",
       "       0.00302645, 0.0071633 , 0.04561354, 0.03911473, 0.00560211,\n",
       "       0.01159241, 0.00823907, 0.00188017, 0.01357042, 0.00944624])), ('param_classifier__C', masked_array(data=[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09,\n",
       "                   0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18,\n",
       "                   0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27,\n",
       "                   0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34,\n",
       "                   0.35000000000000003, 0.36, 0.37, 0.38, 0.39, 0.4,\n",
       "                   0.41000000000000003, 0.42, 0.43, 0.44, 0.45, 0.46,\n",
       "                   0.47000000000000003, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53,\n",
       "                   0.54, 0.55, 0.56, 0.5700000000000001, 0.58, 0.59, 0.6,\n",
       "                   0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68,\n",
       "                   0.6900000000000001, 0.7000000000000001, 0.71, 0.72,\n",
       "                   0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81,\n",
       "                   0.8200000000000001, 0.8300000000000001, 0.84, 0.85,\n",
       "                   0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93,\n",
       "                   0.9400000000000001, 0.9500000000000001, 0.96, 0.97,\n",
       "                   0.98, 0.99, 1.0],\n",
       "             mask=[False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False],\n",
       "       fill_value='?',\n",
       "            dtype=object)), ('params', [{'classifier__C': 0.01}, {'classifier__C': 0.02}, {'classifier__C': 0.03}, {'classifier__C': 0.04}, {'classifier__C': 0.05}, {'classifier__C': 0.06}, {'classifier__C': 0.07}, {'classifier__C': 0.08}, {'classifier__C': 0.09}, {'classifier__C': 0.1}, {'classifier__C': 0.11}, {'classifier__C': 0.12}, {'classifier__C': 0.13}, {'classifier__C': 0.14}, {'classifier__C': 0.15}, {'classifier__C': 0.16}, {'classifier__C': 0.17}, {'classifier__C': 0.18}, {'classifier__C': 0.19}, {'classifier__C': 0.2}, {'classifier__C': 0.21}, {'classifier__C': 0.22}, {'classifier__C': 0.23}, {'classifier__C': 0.24}, {'classifier__C': 0.25}, {'classifier__C': 0.26}, {'classifier__C': 0.27}, {'classifier__C': 0.28}, {'classifier__C': 0.29}, {'classifier__C': 0.3}, {'classifier__C': 0.31}, {'classifier__C': 0.32}, {'classifier__C': 0.33}, {'classifier__C': 0.34}, {'classifier__C': 0.35000000000000003}, {'classifier__C': 0.36}, {'classifier__C': 0.37}, {'classifier__C': 0.38}, {'classifier__C': 0.39}, {'classifier__C': 0.4}, {'classifier__C': 0.41000000000000003}, {'classifier__C': 0.42}, {'classifier__C': 0.43}, {'classifier__C': 0.44}, {'classifier__C': 0.45}, {'classifier__C': 0.46}, {'classifier__C': 0.47000000000000003}, {'classifier__C': 0.48}, {'classifier__C': 0.49}, {'classifier__C': 0.5}, {'classifier__C': 0.51}, {'classifier__C': 0.52}, {'classifier__C': 0.53}, {'classifier__C': 0.54}, {'classifier__C': 0.55}, {'classifier__C': 0.56}, {'classifier__C': 0.5700000000000001}, {'classifier__C': 0.58}, {'classifier__C': 0.59}, {'classifier__C': 0.6}, {'classifier__C': 0.61}, {'classifier__C': 0.62}, {'classifier__C': 0.63}, {'classifier__C': 0.64}, {'classifier__C': 0.65}, {'classifier__C': 0.66}, {'classifier__C': 0.67}, {'classifier__C': 0.68}, {'classifier__C': 0.6900000000000001}, {'classifier__C': 0.7000000000000001}, {'classifier__C': 0.71}, {'classifier__C': 0.72}, {'classifier__C': 0.73}, {'classifier__C': 0.74}, {'classifier__C': 0.75}, {'classifier__C': 0.76}, {'classifier__C': 0.77}, {'classifier__C': 0.78}, {'classifier__C': 0.79}, {'classifier__C': 0.8}, {'classifier__C': 0.81}, {'classifier__C': 0.8200000000000001}, {'classifier__C': 0.8300000000000001}, {'classifier__C': 0.84}, {'classifier__C': 0.85}, {'classifier__C': 0.86}, {'classifier__C': 0.87}, {'classifier__C': 0.88}, {'classifier__C': 0.89}, {'classifier__C': 0.9}, {'classifier__C': 0.91}, {'classifier__C': 0.92}, {'classifier__C': 0.93}, {'classifier__C': 0.9400000000000001}, {'classifier__C': 0.9500000000000001}, {'classifier__C': 0.96}, {'classifier__C': 0.97}, {'classifier__C': 0.98}, {'classifier__C': 0.99}, {'classifier__C': 1.0}]), ('split0_test_score', array([0.93872229, 0.9452412 , 0.94132986, 0.94263364, 0.9452412 ,\n",
       "       0.94784876, 0.94915254, 0.94784876, 0.94654498, 0.94915254,\n",
       "       0.94915254, 0.94915254, 0.95045632, 0.9517601 , 0.9517601 ,\n",
       "       0.95045632, 0.9517601 , 0.9517601 , 0.9517601 , 0.9517601 ,\n",
       "       0.9517601 , 0.95306389, 0.95436767, 0.95436767, 0.95436767,\n",
       "       0.95436767, 0.95436767, 0.95436767, 0.95436767, 0.95436767,\n",
       "       0.95436767, 0.95436767, 0.95306389, 0.95306389, 0.95306389,\n",
       "       0.95306389, 0.9517601 , 0.95045632, 0.9517601 , 0.9517601 ,\n",
       "       0.9517601 , 0.9517601 , 0.9517601 , 0.9517601 , 0.9517601 ,\n",
       "       0.9517601 , 0.9517601 , 0.9517601 , 0.9517601 , 0.9517601 ,\n",
       "       0.9517601 , 0.9517601 , 0.9517601 , 0.9517601 , 0.95045632,\n",
       "       0.95045632, 0.95045632, 0.95045632, 0.95045632, 0.95045632,\n",
       "       0.95045632, 0.95045632, 0.95045632, 0.95045632, 0.95045632,\n",
       "       0.95045632, 0.95045632, 0.95045632, 0.95045632, 0.95045632,\n",
       "       0.95045632, 0.95045632, 0.95045632, 0.95045632, 0.95045632,\n",
       "       0.95045632, 0.95045632, 0.95045632, 0.95045632, 0.95045632,\n",
       "       0.9517601 , 0.95045632, 0.95045632, 0.94915254, 0.95045632,\n",
       "       0.95045632, 0.95045632, 0.95045632, 0.94915254, 0.95045632,\n",
       "       0.95045632, 0.95045632, 0.94915254, 0.95045632, 0.94915254,\n",
       "       0.94915254, 0.95045632, 0.95045632, 0.95045632, 0.95045632])), ('split1_test_score', array([0.92307692, 0.93741851, 0.94132986, 0.94002608, 0.94132986,\n",
       "       0.94002608, 0.94002608, 0.94393742, 0.94393742, 0.94393742,\n",
       "       0.94132986, 0.94002608, 0.93872229, 0.94002608, 0.94002608,\n",
       "       0.94002608, 0.94002608, 0.94002608, 0.94002608, 0.93872229,\n",
       "       0.93872229, 0.93741851, 0.93741851, 0.93741851, 0.93741851,\n",
       "       0.93611473, 0.93611473, 0.93611473, 0.93611473, 0.93611473,\n",
       "       0.93611473, 0.93611473, 0.93741851, 0.93611473, 0.93611473,\n",
       "       0.93481095, 0.93481095, 0.93481095, 0.93611473, 0.93741851,\n",
       "       0.93741851, 0.93741851, 0.93741851, 0.93741851, 0.93741851,\n",
       "       0.93872229, 0.93872229, 0.93872229, 0.93872229, 0.93872229,\n",
       "       0.93872229, 0.93872229, 0.93741851, 0.93872229, 0.93872229,\n",
       "       0.93872229, 0.94002608, 0.94002608, 0.94002608, 0.94002608,\n",
       "       0.94002608, 0.94002608, 0.94132986, 0.94132986, 0.94132986,\n",
       "       0.94132986, 0.94132986, 0.94132986, 0.94132986, 0.94132986,\n",
       "       0.94002608, 0.94002608, 0.94002608, 0.94002608, 0.94132986,\n",
       "       0.94132986, 0.94132986, 0.94132986, 0.94132986, 0.94132986,\n",
       "       0.94132986, 0.94132986, 0.94132986, 0.94132986, 0.94132986,\n",
       "       0.94132986, 0.94132986, 0.94132986, 0.94132986, 0.94132986,\n",
       "       0.94132986, 0.94132986, 0.94132986, 0.94132986, 0.94132986,\n",
       "       0.94132986, 0.94002608, 0.94002608, 0.94002608, 0.93872229])), ('split2_test_score', array([0.93350717, 0.94784876, 0.94915254, 0.95567145, 0.95697523,\n",
       "       0.95827901, 0.95958279, 0.95958279, 0.96088657, 0.96088657,\n",
       "       0.96088657, 0.96088657, 0.96088657, 0.96088657, 0.96088657,\n",
       "       0.96088657, 0.96088657, 0.95958279, 0.95958279, 0.95958279,\n",
       "       0.95697523, 0.95567145, 0.95436767, 0.95436767, 0.95436767,\n",
       "       0.95436767, 0.95436767, 0.95436767, 0.95567145, 0.95567145,\n",
       "       0.95567145, 0.95567145, 0.95567145, 0.95567145, 0.95567145,\n",
       "       0.95567145, 0.95567145, 0.95567145, 0.95567145, 0.95567145,\n",
       "       0.95567145, 0.95697523, 0.95697523, 0.95697523, 0.95697523,\n",
       "       0.95697523, 0.95567145, 0.95567145, 0.95567145, 0.95567145,\n",
       "       0.95567145, 0.95567145, 0.95567145, 0.95567145, 0.95567145,\n",
       "       0.95567145, 0.95567145, 0.95567145, 0.95567145, 0.95567145,\n",
       "       0.95567145, 0.95567145, 0.95567145, 0.95567145, 0.95567145,\n",
       "       0.95567145, 0.95567145, 0.95436767, 0.95306389, 0.95306389,\n",
       "       0.95306389, 0.95306389, 0.95306389, 0.95306389, 0.95306389,\n",
       "       0.95306389, 0.95306389, 0.95306389, 0.9517601 , 0.9517601 ,\n",
       "       0.9517601 , 0.95306389, 0.95306389, 0.95306389, 0.95306389,\n",
       "       0.95306389, 0.95306389, 0.95306389, 0.95436767, 0.95306389,\n",
       "       0.95436767, 0.95436767, 0.95436767, 0.95436767, 0.95436767,\n",
       "       0.95436767, 0.95436767, 0.95436767, 0.95306389, 0.95436767])), ('split3_test_score', array([0.92959583, 0.93741851, 0.93741851, 0.94132986, 0.94393742,\n",
       "       0.9452412 , 0.94263364, 0.94132986, 0.94263364, 0.9452412 ,\n",
       "       0.9452412 , 0.94654498, 0.94654498, 0.94654498, 0.94654498,\n",
       "       0.94654498, 0.94654498, 0.94654498, 0.94784876, 0.94784876,\n",
       "       0.94915254, 0.94915254, 0.94915254, 0.94915254, 0.94915254,\n",
       "       0.94915254, 0.94915254, 0.94915254, 0.94784876, 0.94784876,\n",
       "       0.94784876, 0.94654498, 0.94654498, 0.94654498, 0.94654498,\n",
       "       0.94654498, 0.94654498, 0.94654498, 0.94654498, 0.94654498,\n",
       "       0.94654498, 0.94654498, 0.9452412 , 0.9452412 , 0.9452412 ,\n",
       "       0.9452412 , 0.9452412 , 0.9452412 , 0.9452412 , 0.9452412 ,\n",
       "       0.9452412 , 0.9452412 , 0.9452412 , 0.9452412 , 0.94393742,\n",
       "       0.94393742, 0.94393742, 0.94393742, 0.94393742, 0.94654498,\n",
       "       0.9452412 , 0.94654498, 0.94654498, 0.94654498, 0.94654498,\n",
       "       0.94654498, 0.94654498, 0.94654498, 0.94654498, 0.94654498,\n",
       "       0.94654498, 0.94654498, 0.94654498, 0.94654498, 0.9452412 ,\n",
       "       0.94654498, 0.94654498, 0.94784876, 0.94784876, 0.94784876,\n",
       "       0.94654498, 0.94654498, 0.94654498, 0.94654498, 0.94654498,\n",
       "       0.94654498, 0.94654498, 0.94654498, 0.94654498, 0.94654498,\n",
       "       0.94654498, 0.94654498, 0.94654498, 0.9452412 , 0.9452412 ,\n",
       "       0.9452412 , 0.94654498, 0.94654498, 0.94654498, 0.94654498])), ('split4_test_score', array([0.90861619, 0.92819843, 0.9308094 , 0.93342037, 0.93472585,\n",
       "       0.93472585, 0.93603133, 0.93472585, 0.93472585, 0.93733681,\n",
       "       0.93994778, 0.93994778, 0.93994778, 0.94125326, 0.94386423,\n",
       "       0.94255875, 0.94255875, 0.94255875, 0.94386423, 0.94255875,\n",
       "       0.94255875, 0.94386423, 0.94386423, 0.94255875, 0.94255875,\n",
       "       0.94255875, 0.94125326, 0.94125326, 0.93994778, 0.93994778,\n",
       "       0.94125326, 0.94125326, 0.93994778, 0.94125326, 0.94255875,\n",
       "       0.94255875, 0.94255875, 0.94255875, 0.94125326, 0.94125326,\n",
       "       0.94125326, 0.94125326, 0.94125326, 0.94125326, 0.94125326,\n",
       "       0.94125326, 0.94125326, 0.93994778, 0.93994778, 0.93994778,\n",
       "       0.93994778, 0.9386423 , 0.9386423 , 0.9386423 , 0.9386423 ,\n",
       "       0.93733681, 0.93733681, 0.93733681, 0.93733681, 0.93733681,\n",
       "       0.93733681, 0.93733681, 0.93733681, 0.93733681, 0.93733681,\n",
       "       0.93733681, 0.93733681, 0.93733681, 0.93733681, 0.93733681,\n",
       "       0.93733681, 0.93733681, 0.9386423 , 0.9386423 , 0.9386423 ,\n",
       "       0.9386423 , 0.9386423 , 0.9386423 , 0.9386423 , 0.9386423 ,\n",
       "       0.9386423 , 0.9386423 , 0.9386423 , 0.9386423 , 0.9386423 ,\n",
       "       0.9386423 , 0.93733681, 0.93733681, 0.93733681, 0.93733681,\n",
       "       0.93733681, 0.93733681, 0.93733681, 0.93603133, 0.93603133,\n",
       "       0.93603133, 0.93603133, 0.93603133, 0.93603133, 0.93603133])), ('mean_test_score', array([0.92670368, 0.93922508, 0.94000803, 0.94261628, 0.94444191,\n",
       "       0.94522418, 0.94548528, 0.94548494, 0.94574569, 0.94731091,\n",
       "       0.94731159, 0.94731159, 0.94731159, 0.9480942 , 0.94861639,\n",
       "       0.94809454, 0.9483553 , 0.94809454, 0.94861639, 0.94809454,\n",
       "       0.94783378, 0.94783412, 0.94783412, 0.94757303, 0.94757303,\n",
       "       0.94731227, 0.94705117, 0.94705117, 0.94679008, 0.94679008,\n",
       "       0.94705117, 0.94679042, 0.94652932, 0.94652966, 0.94679076,\n",
       "       0.94653   , 0.94626925, 0.94600849, 0.94626891, 0.94652966,\n",
       "       0.94652966, 0.94679042, 0.94652966, 0.94652966, 0.94652966,\n",
       "       0.94679042, 0.94652966, 0.94626857, 0.94626857, 0.94626857,\n",
       "       0.94626857, 0.94600747, 0.94574671, 0.94600747, 0.94548596,\n",
       "       0.94522486, 0.94548562, 0.94548562, 0.94548562, 0.94600713,\n",
       "       0.94574637, 0.94600713, 0.94626788, 0.94626788, 0.94626788,\n",
       "       0.94626788, 0.94626788, 0.94600713, 0.94574637, 0.94574637,\n",
       "       0.94548562, 0.94548562, 0.94574671, 0.94574671, 0.94574671,\n",
       "       0.94600747, 0.94600747, 0.94626822, 0.94600747, 0.94600747,\n",
       "       0.94600747, 0.94600747, 0.94600747, 0.94574671, 0.94600747,\n",
       "       0.94600747, 0.94574637, 0.94574637, 0.94574637, 0.94574637,\n",
       "       0.94600713, 0.94600713, 0.94574637, 0.94548528, 0.94522452,\n",
       "       0.94522452, 0.94548528, 0.94548528, 0.94522452, 0.94522452])), ('std_test_score', array([0.01038662, 0.00690906, 0.00597389, 0.00725764, 0.00723967,\n",
       "       0.00793161, 0.00824015, 0.00824324, 0.00853553, 0.0077825 ,\n",
       "       0.00750949, 0.00768845, 0.00803441, 0.00763383, 0.00722645,\n",
       "       0.00731246, 0.00741456, 0.00697943, 0.00673959, 0.00726582,\n",
       "       0.00650487, 0.00655374, 0.00650165, 0.00667961, 0.00667961,\n",
       "       0.00708413, 0.00727598, 0.00727598, 0.00772269, 0.00772269,\n",
       "       0.00750597, 0.00749637, 0.00710778, 0.00724679, 0.00707342,\n",
       "       0.00747482, 0.00726206, 0.00708138, 0.00702711, 0.00666008,\n",
       "       0.00666008, 0.00702824, 0.00705664, 0.00705664, 0.00705664,\n",
       "       0.00673176, 0.00634642, 0.00658066, 0.00658066, 0.00658066,\n",
       "       0.00658066, 0.00684678, 0.0071379 , 0.00684678, 0.00668475,\n",
       "       0.00696651, 0.00673892, 0.00673892, 0.00673892, 0.00669972,\n",
       "       0.00669908, 0.00669972, 0.00648375, 0.00648375, 0.00648375,\n",
       "       0.00648375, 0.00648375, 0.00611612, 0.00577224, 0.00577224,\n",
       "       0.00599116, 0.00599116, 0.00564902, 0.00564902, 0.00540293,\n",
       "       0.0054037 , 0.0054037 , 0.00545457, 0.0051459 , 0.0051459 ,\n",
       "       0.00534042, 0.0054037 , 0.0054037 , 0.00521074, 0.0054037 ,\n",
       "       0.0054037 , 0.00577224, 0.00577224, 0.00594631, 0.00577224,\n",
       "       0.00611612, 0.00611612, 0.00594631, 0.00648556, 0.00630412,\n",
       "       0.00630412, 0.00669195, 0.00669195, 0.00635782, 0.00692107])), ('rank_test_score', array([100,  99,  98,  97,  96,  95,  85,  89,  78,  17,  14,  14,  14,\n",
       "         7,   1,   4,   3,   4,   1,   4,  10,   8,   8,  11,  11,  13,\n",
       "        18,  18,  25,  25,  18,  22,  35,  29,  21,  27,  36,  48,  37,\n",
       "        29,  29,  24,  29,  29,  29,  22,  28,  38,  38,  38,  38,  49,\n",
       "        65,  49,  79,  90,  80,  80,  80,  60,  70,  60,  43,  43,  43,\n",
       "        43,  43,  60,  70,  70,  80,  80,  65,  65,  65,  49,  49,  42,\n",
       "        49,  49,  59,  49,  49,  65,  49,  49,  70,  70,  70,  70,  60,\n",
       "        60,  70,  85,  91,  91,  85,  85,  91,  91], dtype=int32))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cv_results_.items()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our precision is .82, indicating that of the model's predicted positives, 82% of them were correct. The recall, of .75, indicates only 75% of the All-NBA players in the dataset were classified as All-NBA.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrix for these results we see that we had 20 false postives, and 31 false negatives. These are not ideal, but considering the size of our dataset, may still work for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>799</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1\n",
       "Actual            \n",
       "0          799  20\n",
       "1           32  91"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test, model.predict(nba_filt_test), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see that our L1 penalty reduced the number of covariates from 79 to 34 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we will extract the feature names from the pipeline\n",
    "feature_names = model.best_estimator_.named_steps['col_transform'].get_feature_names_out()\n",
    "coef_df = pd.DataFrame({'coef':model.best_estimator_['classifier'].coef_[0]\n",
    "                        ,'var':feature_names})\n",
    "coef_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df_nz = coef_df[coef_df['coef']!=0]\n",
    "coef_df_nz.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from our coefficient plot that FGA seems to have the highest impact on the log-odds of winning All-NBA, while FT% has the biggest negative impact. Surprisingly we also see that usage % is another of the strongest negative covariates. This bears further study, but for the purposes of this project (prediction), we do not necessarily care about how the model is weighing each of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-39cab9adef8a4877a0168e16617e5bca\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-39cab9adef8a4877a0168e16617e5bca\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-39cab9adef8a4877a0168e16617e5bca\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-add67b26ebb7b0c1891a91e211bde5b5\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"field\": \"var\", \"sort\": \"-y\", \"title\": \"Variable\", \"type\": \"nominal\"}, \"y\": {\"field\": \"coef\", \"title\": \"Coefficient\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-add67b26ebb7b0c1891a91e211bde5b5\": [{\"coef\": -0.0078647263750618, \"var\": \"select__Age\"}, {\"coef\": -0.024154928792161554, \"var\": \"select__GS\"}, {\"coef\": 1.0322507742538367, \"var\": \"select__FGA\"}, {\"coef\": -0.07968432391381353, \"var\": \"select__3P\"}, {\"coef\": 0.1306121881808973, \"var\": \"select__2P\"}, {\"coef\": 0.18894367647137558, \"var\": \"select__2PA\"}, {\"coef\": 0.041222486572675986, \"var\": \"select__FTA\"}, {\"coef\": -0.5482814888514729, \"var\": \"select__FT%\"}, {\"coef\": 0.05095491292122113, \"var\": \"select__ORB\"}, {\"coef\": 0.09745896258632929, \"var\": \"select__DRB\"}, {\"coef\": 0.04501765377153543, \"var\": \"select__AST\"}, {\"coef\": -0.0228808801558665, \"var\": \"select__STL\"}, {\"coef\": 0.09278997545210793, \"var\": \"select__BLK\"}, {\"coef\": 0.6510945855823826, \"var\": \"select__TOV\"}, {\"coef\": -0.27754637174743046, \"var\": \"select__PF\"}, {\"coef\": 0.8220157039676207, \"var\": \"select__PER\"}, {\"coef\": 0.3056729957658985, \"var\": \"select__3PAr\"}, {\"coef\": 0.21714066503274698, \"var\": \"select__FTr\"}, {\"coef\": -0.17561228881882882, \"var\": \"select__USG%\"}, {\"coef\": 0.3692079091040738, \"var\": \"select__OWS\"}, {\"coef\": 0.22790011466763369, \"var\": \"select__DWS\"}, {\"coef\": 0.6931038889359243, \"var\": \"select__WS\"}, {\"coef\": 0.014222435124414356, \"var\": \"select__WS/48\"}, {\"coef\": 0.2993399789099318, \"var\": \"select__VORP\"}, {\"coef\": 1.091181661526376, \"var\": \"select__W\"}, {\"coef\": 0.0936039383529844, \"var\": \"select__num_all_nba\"}, {\"coef\": 0.3275748275548134, \"var\": \"ohe__Tm_DET\"}, {\"coef\": 0.006360607667550587, \"var\": \"ohe__Tm_MIL\"}, {\"coef\": 0.08796687188804134, \"var\": \"ohe__Tm_OKC\"}, {\"coef\": 0.005647855423198174, \"var\": \"ohe__Tm_PHX\"}, {\"coef\": -0.04972751424928845, \"var\": \"ohe__Tm_POR\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we will make a bar chart of these coefficients\n",
    "alt.Chart(coef_df_nz).mark_bar().encode(\n",
    "    y=alt.Y('coef',title='Coefficient'),\n",
    "    x=alt.X('var',title='Variable', sort = '-y'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for this dataset we have the following players who were predicted All-NBA but did not win it. (False Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>Ja Morant</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Ben Simmons</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>James Harden</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Trae Young</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>James Harden</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>Chauncey Billups</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>Horace Grant</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>Andrei Kirilenko</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>Karl Malone</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>Josh Smith</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>Elton Brand</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>Kevin Durant</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>Marques Johnson</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>Pau Gasol</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>Rudy Gobert</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>Tim Duncan</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>Shaquille O'Neal</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>Alonzo Mourning</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3414</th>\n",
       "      <td>Robert Parish</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>Karl-Anthony Towns</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Player  year\n",
       "463            Ja Morant  2023\n",
       "514          Ben Simmons  2019\n",
       "577         James Harden  2023\n",
       "794           Trae Young  2021\n",
       "851         James Harden  2021\n",
       "1152    Chauncey Billups  2008\n",
       "1803        Horace Grant  1992\n",
       "1951    Andrei Kirilenko  2004\n",
       "1995         Karl Malone  2003\n",
       "2285          Josh Smith  2012\n",
       "2347         Elton Brand  2002\n",
       "2591        Kevin Durant  2023\n",
       "2755     Marques Johnson  1983\n",
       "2983           Pau Gasol  2012\n",
       "3050         Rudy Gobert  2022\n",
       "3063          Tim Duncan  2012\n",
       "3326    Shaquille O'Neal  1993\n",
       "3400     Alonzo Mourning  1997\n",
       "3414       Robert Parish  1984\n",
       "3419  Karl-Anthony Towns  2017"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_filt_test[(model.predict(nba_filt_test)==1) & (y_test!=1)][['Player',\"year\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our False Negatives we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gary Payton</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Joe Dumars</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Joe Dumars</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Mitch Richmond</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>Mitch Richmond</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>Isiah Thomas</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>Chauncey Billups</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>Tim Hardaway</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>Manu Ginbili</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>Stephon Marbury</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>Reggie Miller</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>Ben Simmons</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>Mark Price</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>Eddie Jones</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>Mitch Richmond</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>Jermaine O'Neal</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>Tom Chambers</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>Pascal Siakam</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>Paul Pierce</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>Dennis Rodman</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>Draymond Green</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>Jimmy Butler</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>Bernard King</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>Chris Mullin</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>Shaquille O'Neal</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>Yao Ming</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>Patrick Ewing</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>Ben Wallace</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358</th>\n",
       "      <td>Dwight Howard</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>Marc Gasol</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>Joel Embiid</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>DeMarcus Cousins</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Player  year\n",
       "23         Gary Payton  1994\n",
       "284         Joe Dumars  1990\n",
       "304         Joe Dumars  1993\n",
       "314     Mitch Richmond  1996\n",
       "652     Mitch Richmond  1997\n",
       "717       Isiah Thomas  1987\n",
       "1010  Chauncey Billups  2009\n",
       "1055      Tim Hardaway  1993\n",
       "1079     Manu Ginbili  2011\n",
       "1093   Stephon Marbury  2003\n",
       "1247     Reggie Miller  1996\n",
       "1446       Ben Simmons  2020\n",
       "1462        Mark Price  1989\n",
       "1463       Eddie Jones  2000\n",
       "1493    Mitch Richmond  1995\n",
       "1862   Jermaine O'Neal  2002\n",
       "2105      Tom Chambers  1989\n",
       "2359     Pascal Siakam  2020\n",
       "2448       Paul Pierce  2009\n",
       "2452     Dennis Rodman  1995\n",
       "2491    Draymond Green  2017\n",
       "2590      Jimmy Butler  2018\n",
       "2845      Bernard King  1991\n",
       "2861      Chris Mullin  1989\n",
       "3005  Shaquille O'Neal  2009\n",
       "3067          Yao Ming  2004\n",
       "3098     Patrick Ewing  1988\n",
       "3312       Ben Wallace  2006\n",
       "3358     Dwight Howard  2014\n",
       "3375        Marc Gasol  2013\n",
       "3377       Joel Embiid  2018\n",
       "3684  DeMarcus Cousins  2016"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_filt_test[(model.predict(nba_filt_test)!=1) & (y_test==1)][['Player',\"year\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may consider fitting position specific models. We will use the same `MP` and `G` filters we used prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_g_train = pd.read_csv('Data_Scripting_Cleaning/Full_data/Training_Sets/nba_g_train.csv')\n",
    "nba_g_test = pd.read_csv('Data_Scripting_Cleaning/Full_data/Test_Sets/nba_g_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_g_filt_train = nba_g_train[(nba_g_train['MP']>=min_minutes) & (nba_g_train['G']>=min_G)]\n",
    "nba_g_filt_test = nba_g_test[(nba_g_test['MP']>=min_minutes) & (nba_g_test['G']>=min_G)]\n",
    "y_g_train = nba_g_filt_train['all_nba_c_year']\n",
    "y_g_test = nba_g_filt_test['all_nba_c_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;col_transform&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;select&#x27;,\n",
       "                                                                         &#x27;passthrough&#x27;,\n",
       "                                                                         [&#x27;Age&#x27;,\n",
       "                                                                          &#x27;G&#x27;,\n",
       "                                                                          &#x27;GS&#x27;,\n",
       "                                                                          &#x27;MP&#x27;,\n",
       "                                                                          &#x27;FG&#x27;,\n",
       "                                                                          &#x27;FGA&#x27;,\n",
       "                                                                          &#x27;FG%&#x27;,\n",
       "                                                                          &#x27;3P&#x27;,\n",
       "                                                                          &#x27;3PA&#x27;,\n",
       "                                                                          &#x27;3P%&#x27;,\n",
       "                                                                          &#x27;2P&#x27;,\n",
       "                                                                          &#x27;2PA&#x27;,\n",
       "                                                                          &#x27;2P%&#x27;,\n",
       "                                                                          &#x27;eFG%&#x27;,\n",
       "                                                                          &#x27;FT&#x27;,\n",
       "                                                                          &#x27;FTA&#x27;,\n",
       "                                                                          &#x27;FT%&#x27;,\n",
       "                                                                          &#x27;ORB&#x27;,\n",
       "                                                                          &#x27;DRB&#x27;,\n",
       "                                                                          &#x27;TRB&#x27;,\n",
       "                                                                          &#x27;AST&#x27;,\n",
       "                                                                          &#x27;STL&#x27;,\n",
       "                                                                          &#x27;BLK&#x27;,\n",
       "                                                                          &#x27;TOV&#x27;,\n",
       "                                                                          &#x27;PF&#x27;,\n",
       "                                                                          &#x27;PTS&#x27;,\n",
       "                                                                          &#x27;PER&#x27;,\n",
       "                                                                          &#x27;TS%&#x27;,\n",
       "                                                                          &#x27;3PAr&#x27;,\n",
       "                                                                          &#x27;FTr&#x27;, ...]),\n",
       "                                                                        (&#x27;ohe&#x27;,\n",
       "                                                                         OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                                         [&#x27;Tm&#x27;])])),\n",
       "                                       (...&#x27;,\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           penalty=&#x27;l1&#x27;,\n",
       "                                                           random_state=0,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             param_grid={&#x27;classifier__C&#x27;: array([1.00000000e-04, 3.72759372e-04, 1.38949549e-03, 5.17947468e-03,\n",
       "       1.93069773e-02, 7.19685673e-02, 2.68269580e-01, 1.00000000e+00,\n",
       "       3.72759372e+00, 1.38949549e+01, 5.17947468e+01, 1.93069773e+02,\n",
       "       7.19685673e+02, 2.68269580e+03, 1.00000000e+04])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;col_transform&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;select&#x27;,\n",
       "                                                                         &#x27;passthrough&#x27;,\n",
       "                                                                         [&#x27;Age&#x27;,\n",
       "                                                                          &#x27;G&#x27;,\n",
       "                                                                          &#x27;GS&#x27;,\n",
       "                                                                          &#x27;MP&#x27;,\n",
       "                                                                          &#x27;FG&#x27;,\n",
       "                                                                          &#x27;FGA&#x27;,\n",
       "                                                                          &#x27;FG%&#x27;,\n",
       "                                                                          &#x27;3P&#x27;,\n",
       "                                                                          &#x27;3PA&#x27;,\n",
       "                                                                          &#x27;3P%&#x27;,\n",
       "                                                                          &#x27;2P&#x27;,\n",
       "                                                                          &#x27;2PA&#x27;,\n",
       "                                                                          &#x27;2P%&#x27;,\n",
       "                                                                          &#x27;eFG%&#x27;,\n",
       "                                                                          &#x27;FT&#x27;,\n",
       "                                                                          &#x27;FTA&#x27;,\n",
       "                                                                          &#x27;FT%&#x27;,\n",
       "                                                                          &#x27;ORB&#x27;,\n",
       "                                                                          &#x27;DRB&#x27;,\n",
       "                                                                          &#x27;TRB&#x27;,\n",
       "                                                                          &#x27;AST&#x27;,\n",
       "                                                                          &#x27;STL&#x27;,\n",
       "                                                                          &#x27;BLK&#x27;,\n",
       "                                                                          &#x27;TOV&#x27;,\n",
       "                                                                          &#x27;PF&#x27;,\n",
       "                                                                          &#x27;PTS&#x27;,\n",
       "                                                                          &#x27;PER&#x27;,\n",
       "                                                                          &#x27;TS%&#x27;,\n",
       "                                                                          &#x27;3PAr&#x27;,\n",
       "                                                                          &#x27;FTr&#x27;, ...]),\n",
       "                                                                        (&#x27;ohe&#x27;,\n",
       "                                                                         OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                                         [&#x27;Tm&#x27;])])),\n",
       "                                       (...&#x27;,\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           penalty=&#x27;l1&#x27;,\n",
       "                                                           random_state=0,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             param_grid={&#x27;classifier__C&#x27;: array([1.00000000e-04, 3.72759372e-04, 1.38949549e-03, 5.17947468e-03,\n",
       "       1.93069773e-02, 7.19685673e-02, 2.68269580e-01, 1.00000000e+00,\n",
       "       3.72759372e+00, 1.38949549e+01, 5.17947468e+01, 1.93069773e+02,\n",
       "       7.19685673e+02, 2.68269580e+03, 1.00000000e+04])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;col_transform&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;select&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [&#x27;Age&#x27;, &#x27;G&#x27;, &#x27;GS&#x27;, &#x27;MP&#x27;, &#x27;FG&#x27;,\n",
       "                                                   &#x27;FGA&#x27;, &#x27;FG%&#x27;, &#x27;3P&#x27;, &#x27;3PA&#x27;,\n",
       "                                                   &#x27;3P%&#x27;, &#x27;2P&#x27;, &#x27;2PA&#x27;, &#x27;2P%&#x27;,\n",
       "                                                   &#x27;eFG%&#x27;, &#x27;FT&#x27;, &#x27;FTA&#x27;, &#x27;FT%&#x27;,\n",
       "                                                   &#x27;ORB&#x27;, &#x27;DRB&#x27;, &#x27;TRB&#x27;, &#x27;AST&#x27;,\n",
       "                                                   &#x27;STL&#x27;, &#x27;BLK&#x27;, &#x27;TOV&#x27;, &#x27;PF&#x27;,\n",
       "                                                   &#x27;PTS&#x27;, &#x27;PER&#x27;, &#x27;TS%&#x27;, &#x27;3PAr&#x27;,\n",
       "                                                   &#x27;FTr&#x27;, ...]),\n",
       "                                                 (&#x27;ohe&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;Tm&#x27;])])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(max_iter=10000, penalty=&#x27;l1&#x27;,\n",
       "                                    random_state=0, solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">col_transform: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;select&#x27;, &#x27;passthrough&#x27;,\n",
       "                                 [&#x27;Age&#x27;, &#x27;G&#x27;, &#x27;GS&#x27;, &#x27;MP&#x27;, &#x27;FG&#x27;, &#x27;FGA&#x27;, &#x27;FG%&#x27;,\n",
       "                                  &#x27;3P&#x27;, &#x27;3PA&#x27;, &#x27;3P%&#x27;, &#x27;2P&#x27;, &#x27;2PA&#x27;, &#x27;2P%&#x27;,\n",
       "                                  &#x27;eFG%&#x27;, &#x27;FT&#x27;, &#x27;FTA&#x27;, &#x27;FT%&#x27;, &#x27;ORB&#x27;, &#x27;DRB&#x27;,\n",
       "                                  &#x27;TRB&#x27;, &#x27;AST&#x27;, &#x27;STL&#x27;, &#x27;BLK&#x27;, &#x27;TOV&#x27;, &#x27;PF&#x27;,\n",
       "                                  &#x27;PTS&#x27;, &#x27;PER&#x27;, &#x27;TS%&#x27;, &#x27;3PAr&#x27;, &#x27;FTr&#x27;, ...]),\n",
       "                                (&#x27;ohe&#x27;, OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;Tm&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">select</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Age&#x27;, &#x27;G&#x27;, &#x27;GS&#x27;, &#x27;MP&#x27;, &#x27;FG&#x27;, &#x27;FGA&#x27;, &#x27;FG%&#x27;, &#x27;3P&#x27;, &#x27;3PA&#x27;, &#x27;3P%&#x27;, &#x27;2P&#x27;, &#x27;2PA&#x27;, &#x27;2P%&#x27;, &#x27;eFG%&#x27;, &#x27;FT&#x27;, &#x27;FTA&#x27;, &#x27;FT%&#x27;, &#x27;ORB&#x27;, &#x27;DRB&#x27;, &#x27;TRB&#x27;, &#x27;AST&#x27;, &#x27;STL&#x27;, &#x27;BLK&#x27;, &#x27;TOV&#x27;, &#x27;PF&#x27;, &#x27;PTS&#x27;, &#x27;PER&#x27;, &#x27;TS%&#x27;, &#x27;3PAr&#x27;, &#x27;FTr&#x27;, &#x27;ORB%&#x27;, &#x27;DRB%&#x27;, &#x27;TRB%&#x27;, &#x27;AST%&#x27;, &#x27;STL%&#x27;, &#x27;BLK%&#x27;, &#x27;TOV%&#x27;, &#x27;USG%&#x27;, &#x27;OWS&#x27;, &#x27;DWS&#x27;, &#x27;WS&#x27;, &#x27;WS/48&#x27;, &#x27;OBPM&#x27;, &#x27;DBPM&#x27;, &#x27;BPM&#x27;, &#x27;VORP&#x27;, &#x27;W&#x27;, &#x27;num_all_nba&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ohe</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Tm&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, penalty=&#x27;l1&#x27;, random_state=0,\n",
       "                   solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('col_transform',\n",
       "                                        ColumnTransformer(transformers=[('select',\n",
       "                                                                         'passthrough',\n",
       "                                                                         ['Age',\n",
       "                                                                          'G',\n",
       "                                                                          'GS',\n",
       "                                                                          'MP',\n",
       "                                                                          'FG',\n",
       "                                                                          'FGA',\n",
       "                                                                          'FG%',\n",
       "                                                                          '3P',\n",
       "                                                                          '3PA',\n",
       "                                                                          '3P%',\n",
       "                                                                          '2P',\n",
       "                                                                          '2PA',\n",
       "                                                                          '2P%',\n",
       "                                                                          'eFG%',\n",
       "                                                                          'FT',\n",
       "                                                                          'FTA',\n",
       "                                                                          'FT%',\n",
       "                                                                          'ORB',\n",
       "                                                                          'DRB',\n",
       "                                                                          'TRB',\n",
       "                                                                          'AST',\n",
       "                                                                          'STL',\n",
       "                                                                          'BLK',\n",
       "                                                                          'TOV',\n",
       "                                                                          'PF',\n",
       "                                                                          'PTS',\n",
       "                                                                          'PER',\n",
       "                                                                          'TS%',\n",
       "                                                                          '3PAr',\n",
       "                                                                          'FTr', ...]),\n",
       "                                                                        ('ohe',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                         ['Tm'])])),\n",
       "                                       (...',\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           penalty='l1',\n",
       "                                                           random_state=0,\n",
       "                                                           solver='liblinear'))]),\n",
       "             param_grid={'classifier__C': array([1.00000000e-04, 3.72759372e-04, 1.38949549e-03, 5.17947468e-03,\n",
       "       1.93069773e-02, 7.19685673e-02, 2.68269580e-01, 1.00000000e+00,\n",
       "       3.72759372e+00, 1.38949549e+01, 5.17947468e+01, 1.93069773e+02,\n",
       "       7.19685673e+02, 2.68269580e+03, 1.00000000e+04])})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now I will create my pipeline as before, but I will include a step to remove a subset of variables I specify\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "clf_g = Pipeline([\n",
    "    (\"col_transform\", ct),\n",
    "    (\"classifier\", LogisticRegression(penalty = 'l1', solver = 'liblinear', \n",
    "                                      max_iter = 10000, random_state=0))\n",
    "])\n",
    "\n",
    "model_g = GridSearchCV(clf_g, param_grid)\n",
    "model_g.fit(nba_g_filt_train, y_g_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at only guards we have similar accuracy, with worse recall for true positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       369\n",
      "           1       0.82      0.65      0.73        49\n",
      "\n",
      "    accuracy                           0.94       418\n",
      "   macro avg       0.89      0.82      0.85       418\n",
      "weighted avg       0.94      0.94      0.94       418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_g_test, model_g.predict(nba_g_filt_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>362</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1\n",
       "Actual            \n",
       "0          362   7\n",
       "1           17  32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_g_test, model_g.predict(nba_g_filt_test), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our false positives as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>Ja Morant</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>James Harden</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>Jimmy Butler</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>James Harden</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>Chauncey Billups</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>Kevin Johnson</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>John Stockton</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Player  year\n",
       "463          Ja Morant  2023\n",
       "577       James Harden  2023\n",
       "774       Jimmy Butler  2015\n",
       "851       James Harden  2021\n",
       "1152  Chauncey Billups  2008\n",
       "1187     Kevin Johnson  1997\n",
       "1250     John Stockton  2000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_g_filt_test[(model_g.predict(nba_g_filt_test)==1) & (y_g_test!=1)][['Player','year']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our false negatives as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gary Payton</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Joe Dumars</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Joe Dumars</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Mitch Richmond</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>Mitch Richmond</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Deron Williams</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>Isiah Thomas</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>Chauncey Billups</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>Tim Hardaway</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>Manu Ginbili</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>Stephon Marbury</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>Reggie Miller</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>Ben Simmons</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>Mark Price</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>Eddie Jones</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>Mitch Richmond</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>Damian Lillard</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Player  year\n",
       "23         Gary Payton  1994\n",
       "284         Joe Dumars  1990\n",
       "304         Joe Dumars  1993\n",
       "314     Mitch Richmond  1996\n",
       "652     Mitch Richmond  1997\n",
       "673     Deron Williams  2010\n",
       "717       Isiah Thomas  1987\n",
       "1010  Chauncey Billups  2009\n",
       "1055      Tim Hardaway  1993\n",
       "1079     Manu Ginbili  2011\n",
       "1093   Stephon Marbury  2003\n",
       "1247     Reggie Miller  1996\n",
       "1446       Ben Simmons  2020\n",
       "1462        Mark Price  1989\n",
       "1463       Eddie Jones  2000\n",
       "1493    Mitch Richmond  1995\n",
       "1495    Damian Lillard  2016"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_g_filt_test[(model_g.predict(nba_g_filt_test)!=1) & (y_g_test==1)][['Player','year']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining our coefficients we see the L1 penalty reduced our number of features by almost 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_g_df = pd.DataFrame({'coef':model_g.best_estimator_['classifier'].coef_[0],\n",
    "                          'var':feature_names})\n",
    "coef_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_g_nz = coef_g_df[coef_g_df['coef']!=0]\n",
    "coef_g_nz.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at their values we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-1c95809a9a994ba7874f5f146ec63021\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1c95809a9a994ba7874f5f146ec63021\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1c95809a9a994ba7874f5f146ec63021\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-cf3fafdb68f0e49cd0562176275824f0\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"field\": \"var\", \"sort\": \"-y\", \"title\": \"Variable\", \"type\": \"nominal\"}, \"y\": {\"field\": \"coef\", \"title\": \"Coefficient\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-cf3fafdb68f0e49cd0562176275824f0\": [{\"coef\": -0.06359906531009402, \"var\": \"select__Age\"}, {\"coef\": -0.20712224326534276, \"var\": \"select__G\"}, {\"coef\": -0.13689757672523045, \"var\": \"select__GS\"}, {\"coef\": 0.8827626103785179, \"var\": \"select__2P\"}, {\"coef\": 0.02265725048126158, \"var\": \"select__FT\"}, {\"coef\": 0.1983474860734365, \"var\": \"select__FTA\"}, {\"coef\": 0.01599009519715554, \"var\": \"select__AST\"}, {\"coef\": -0.03446626182670417, \"var\": \"select__STL\"}, {\"coef\": 0.12194385951613322, \"var\": \"select__TOV\"}, {\"coef\": -0.26660030665739615, \"var\": \"select__PF\"}, {\"coef\": 0.3119856872825398, \"var\": \"select__OWS\"}, {\"coef\": 0.053664717573522244, \"var\": \"select__OBPM\"}, {\"coef\": -0.13527685655714844, \"var\": \"select__DBPM\"}, {\"coef\": 1.3229906299920315, \"var\": \"select__VORP\"}, {\"coef\": 0.9557355644696995, \"var\": \"select__W\"}, {\"coef\": 0.15703630812824437, \"var\": \"select__num_all_nba\"}, {\"coef\": 0.23802226860355338, \"var\": \"ohe__Tm_PHX\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(coef_g_nz).mark_bar().encode(\n",
    "    y=alt.Y('coef',title='Coefficient'),\n",
    "    x=alt.X('var',title='Variable', sort = '-y'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that VORP has one of the largest positive coefficients, while number of personal fouls is one of the largest magnitude negative coefficients. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this analysis for the C's and F's we get:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_F_train = pd.read_csv('Data_Scripting_Cleaning/Full_data/Training_Sets/nba_F_train.csv')\n",
    "nba_F_test = pd.read_csv('Data_Scripting_Cleaning/Full_data/Test_Sets/nba_F_test.csv')\n",
    "\n",
    "nba_F_filt_train = nba_F_train[(nba_F_train['MP']>=min_minutes) & (nba_F_train['G']>=min_G)]\n",
    "nba_F_filt_test = nba_F_test[(nba_F_test['MP']>=min_minutes) & (nba_F_test['G']>=min_G)]\n",
    "\n",
    "\n",
    "y_F_train = nba_F_filt_train['all_nba_c_year']\n",
    "y_F_test = nba_F_filt_test['all_nba_c_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m clf_F \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m      2\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mcol_transform\u001b[39m\u001b[39m\"\u001b[39m, ct),\n\u001b[1;32m      3\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m\"\u001b[39m, LogisticRegression(penalty \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m'\u001b[39m, solver \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                                       max_iter \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      6\u001b[0m model_F \u001b[39m=\u001b[39m GridSearchCV(clf_F, param_grid)\n\u001b[0;32m----> 7\u001b[0m model_F\u001b[39m.\u001b[39;49mfit(nba_F_filt_train, y_F_train)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1216\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[39mif\u001b[39;00m effective_n_jobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1211\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1212\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m > 1 does not have any effect when\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39msolver\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is set to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Got \u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(effective_n_jobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs))\n\u001b[1;32m   1215\u001b[0m         )\n\u001b[0;32m-> 1216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _fit_liblinear(\n\u001b[1;32m   1217\u001b[0m         X,\n\u001b[1;32m   1218\u001b[0m         y,\n\u001b[1;32m   1219\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m   1220\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m   1221\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintercept_scaling,\n\u001b[1;32m   1222\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m   1223\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty,\n\u001b[1;32m   1224\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdual,\n\u001b[1;32m   1225\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1226\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1227\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m   1228\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m   1229\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1230\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m   1233\u001b[0m \u001b[39mif\u001b[39;00m solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39msag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1224\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1221\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m   1223\u001b[0m solver_type \u001b[39m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1224\u001b[0m raw_coef_, n_iter_ \u001b[39m=\u001b[39m liblinear\u001b[39m.\u001b[39;49mtrain_wrap(\n\u001b[1;32m   1225\u001b[0m     X,\n\u001b[1;32m   1226\u001b[0m     y_ind,\n\u001b[1;32m   1227\u001b[0m     sp\u001b[39m.\u001b[39;49misspmatrix(X),\n\u001b[1;32m   1228\u001b[0m     solver_type,\n\u001b[1;32m   1229\u001b[0m     tol,\n\u001b[1;32m   1230\u001b[0m     bias,\n\u001b[1;32m   1231\u001b[0m     C,\n\u001b[1;32m   1232\u001b[0m     class_weight_,\n\u001b[1;32m   1233\u001b[0m     max_iter,\n\u001b[1;32m   1234\u001b[0m     rnd\u001b[39m.\u001b[39;49mrandint(np\u001b[39m.\u001b[39;49miinfo(\u001b[39m\"\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmax),\n\u001b[1;32m   1235\u001b[0m     epsilon,\n\u001b[1;32m   1236\u001b[0m     sample_weight,\n\u001b[1;32m   1237\u001b[0m )\n\u001b[1;32m   1238\u001b[0m \u001b[39m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[39m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[39m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[39m# srand supports\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m n_iter_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_F = Pipeline([\n",
    "    (\"col_transform\", ct),\n",
    "    (\"classifier\", LogisticRegression(penalty = 'l1', solver = 'liblinear', \n",
    "                                      max_iter = 10000, random_state=0))\n",
    "])\n",
    "model_F = GridSearchCV(clf_F, param_grid)\n",
    "model_F.fit(nba_F_filt_train, y_F_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_F_test, model_F.predict(nba_F_filt_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_F_test, model_F.predict(nba_F_filt_test), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_F_df = pd.DataFrame({'coef':model_F.best_estimator_['classifier'].coef_[0],\n",
    "                          'var':feature_names})\n",
    "coef_F_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_F_nz = coef_F_df[coef_F_df['coef']!=0]\n",
    "coef_F_nz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(coef_F_nz).mark_bar().encode(\n",
    "    y=alt.Y('coef',title='Coefficient'),\n",
    "    x=alt.X('var',title='Variable', sort = '-y'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_C_train = pd.read_csv('Data_Scripting_Cleaning/Full_data/Training_Sets/nba_C_train.csv')\n",
    "nba_C_test = pd.read_csv('Data_Scripting_Cleaning/Full_data/Test_Sets/nba_C_test.csv')\n",
    "\n",
    "nba_C_filt_train = nba_C_train[(nba_C_train['MP']>=min_minutes) & (nba_C_train['G']>=min_G)]\n",
    "nba_C_filt_test = nba_C_test[(nba_C_test['MP']>=min_minutes) & (nba_C_test['G']>=min_G)]\n",
    "\n",
    "\n",
    "y_C_train = nba_C_filt_train['all_nba_c_year']\n",
    "y_C_test = nba_C_filt_test['all_nba_c_year']\n",
    "\n",
    "clf_C = Pipeline([\n",
    "    (\"col_transform\", ct),\n",
    "    (\"classifier\", LogisticRegression(penalty = 'l1', solver = 'liblinear', \n",
    "                                      max_iter = 10000, random_state=0))\n",
    "])\n",
    "model_C = GridSearchCV(clf_C, param_grid)\n",
    "model_C.fit(nba_C_filt_train, y_C_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the Centers had a quite poor positive recall in this dataset. This makes sense, since this dataset is the most unbalanced of the three, since only one center is picked for each All-NBA team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_C_test, model_C.predict(nba_C_filt_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_C_test, model_C.predict(nba_C_filt_test), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_C = pd.DataFrame({'coef':model_C.best_estimator_['classifier'].coef_[0],\n",
    "                       'var':feature_names})\n",
    "coef_C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_C_nz = coef_C[coef_C['coef']!=0]\n",
    "coef_C_nz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(coef_C_nz).mark_bar().encode(\n",
    "    y=alt.Y('coef',title='Coefficient'),\n",
    "    x=alt.X('var',title='Variable', sort = '-y'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model Wins and the advanced statistic Win shares were two of the stats with the largest positive impact on the model, while FT% was one of the largest negative coefficients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may try to balance our datasets. First we will try this on the larger dataset. We will first use SMOTENC sampling (Synthetic Minority Oversampling Technique-Numerical, Categorical). This method uses a k-nearest neighbors approach (default k=5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balance all_nba_c_year in the nba_train set\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "smote = SMOTENC(random_state=0,categorical_features=[nba_filt_train[num_features+cat_features].shape[1]-1])\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(nba_filt_train[num_features+cat_features], nba_filt_train['all_nba_c_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bal = Pipeline([\n",
    "    (\"col_transform\", ct),\n",
    "    (\"classifier\", LogisticRegression(penalty = 'l1', solver = 'liblinear', \n",
    "                                      max_iter = 10000, random_state=0))\n",
    "])\n",
    "model_smote = GridSearchCV(clf_bal, param_grid)\n",
    "model_smote.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(nba_filt_test['all_nba_c_year'], model_smote.predict(nba_filt_test[num_features+cat_features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(nba_filt_test['all_nba_c_year'], model_smote.predict(nba_filt_test[num_features+cat_features]), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see using this balanced dataset greatly increases our recall, but at the cost of our precision. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also try under-sampling. This will essentially remove rows of the majority class (those who did not make All-NBA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we may use undersampling to balance the classes\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "ran_uns = RandomUnderSampler(random_state=0)\n",
    "X_train_resampled, y_train_resampled = ran_uns.fit_resample(nba_filt_train[num_features+cat_features], nba_filt_train['all_nba_c_year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_us_bal = Pipeline([\n",
    "    (\"col_transform\", ct),\n",
    "    (\"classifier\", LogisticRegression(penalty = 'l1', solver = 'liblinear', \n",
    "                                      max_iter = 10000, random_state=0))\n",
    "])\n",
    "model_us = GridSearchCV(clf_us_bal, param_grid)\n",
    "model_us.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(nba_filt_test['all_nba_c_year'], model_us.predict(nba_filt_test[num_features+cat_features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(nba_filt_test['all_nba_c_year'], model_us.predict(nba_filt_test[num_features+cat_features]), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can try over-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we may use oversampling to balance the classes\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ran_os = RandomOverSampler(random_state=0)\n",
    "X_train_resampled, y_train_resampled = ran_os.fit_resample(nba_filt_train[num_features+cat_features], nba_filt_train['all_nba_c_year'])\n",
    "clf_os_bal = Pipeline([\n",
    "    (\"col_transform\", ct),\n",
    "    (\"classifier\", LogisticRegression(penalty = 'l1', solver = 'liblinear', \n",
    "                                      max_iter = 10000, random_state=0))\n",
    "])\n",
    "model_os = GridSearchCV(clf_os_bal, param_grid)\n",
    "model_os.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(nba_filt_test['all_nba_c_year'], model_os.predict(nba_filt_test[num_features+cat_features])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(nba_filt_test['all_nba_c_year'], model_os.predict(nba_filt_test[num_features+cat_features]), rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see all of these 3 methods are not significantly better, or are quite worse than the baseline unbalanced class model. Specifically we have quite poor precision for our positive classifications "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
