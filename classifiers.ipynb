{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will first attempt to fit the models to the entire dataset, ignoring positions. This may work, however, we may see trends in our more specific models where certain stats are more impactful for All-NBA statues (i.e. defensive stats may be more important for Centers as opposed to guards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_train = pd.read_csv('Data_Scripting_Cleaning/Full_data/Training_Sets/nba_szn_train.csv')\n",
    "nba_test = pd.read_csv('Data_Scripting_Cleaning/Full_data/Test_Sets/nba_szn_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can examine the proportions of All-NBA in our dataset. We created the test and training set so that 9 random seasons were in the test set, and the rest were in training. This means our proportions should be similar for the classes in each set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.967329\n",
       "1    0.032671\n",
       "Name: all_nba_c_year, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Proportions of all_nba_c_year\n",
    "nba_train['all_nba_c_year'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the testing stat we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.96696\n",
       "1    0.03304\n",
       "Name: all_nba_c_year, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba_test['all_nba_c_year'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sets have similar proportions as expected, but clearly we have an incredibly unbalanced dataset. To account for this, we may tune the `class-weights` parameter which will place more weights on the All-NBA class in order to account for this imbalance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will filter our data based on variables like minutes played (`MP`), games played (`G`), since we know that these awards go to the best players in the NBA, and the best players tend to play a lot. In the new 2023 CBA (collective bargaining agreement), there is a minimum game requirement (65 games) that must be met in order to win All-NBA. However, since this rule was not in place for prior awards (where this data comes from), we can instead filter so that we only consider players who have played more games than the players with the least minutes played and games played that still won All-NBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_minutes = nba_train[(nba_train['all_nba_c_year']==1)].MP.min()\n",
    "min_G = nba_train[(nba_train['all_nba_c_year']==1)].G.min()\n",
    "nba_filt_train = nba_train[(nba_train['MP']>=min_minutes) & (nba_train['G']>=min_G)]\n",
    "nba_filt_test = nba_test[(nba_test['MP']>=min_minutes) & (nba_test['G']>=min_G)]\n",
    "\n",
    "y_train = nba_filt_train['all_nba_c_year']\n",
    "\n",
    "y_test = nba_filt_test['all_nba_c_year']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we may fit a simpler model. Which players will make all-nba, versus which players won't given their current season stats? In this case we will ignore teams and instead only focus on the binary indicator. We may extract predicted teams by ordering the probabilities and constructing the teams in that way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can describe the model of interest under a statistical framework. Denote the following quantities:\n",
    "1) y: an n x 1 vector, containing the binary variable of interest\n",
    "2) X: and n x (p+1) matrix, consisting of the feature variables and intercept\n",
    "3) $\\beta$: A (p+1) x 1 vector of coefficients.\n",
    "\n",
    "We will also use the following functions:\n",
    "\n",
    "1) $logit(p) = log(\\frac{p}{1-p})$; this is often denoted as the log-odds\n",
    "2) $expit(x) = \\frac{1}{1+exp(-x)}$; this is the inverse function of logit, i.e. $expit(x) = logit^{-1}(x)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will be utilizing is:\n",
    "$$\n",
    "y_i \\sim Bernoulli(p_i = expit(x_i^{T}\\beta))\n",
    "$$\n",
    "\n",
    "Where $x_i$ is the i'th row of the feature matrix X."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume independence (clearly broken here since player performance is clearly correlated across different seasons, but we will disregard this for now), then we have a likelihood function of the form:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^n (expit(x_i^{T}\\beta))^{y_i}\\times (1-expit(x_i^{T}\\beta))^{1-y_i}\n",
    "$$\n",
    "\n",
    "Leading to a log-likelihood function (our unregularized negative objective function) of:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = \\sum_{i=1}^n y_i(log(expit(x_i^{T}\\beta))) + (1-y_i)log(1-expit(x_i^{T}\\beta)) \\\\ = \\sum_{i=1}^n y_i(x_i^T\\beta)-log(1+exp(x_i^T\\beta))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we will be finding:\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{min}\\sum_{i=1}^n -y_i(x_i^T\\beta)+log(1+exp(x_i^T\\beta)) + r(\\beta)\n",
    "$$\n",
    "\n",
    "where $r(\\beta)$ is a regularization term"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the L1 regularizer SKlearn specifically will be minimizing:\n",
    "$$\n",
    "\\underset{\\beta}{min} \\ C\\sum_{i=1}^n -y_i(x_i^T\\beta)+log(1+exp(x_i^T\\beta)) + \\sum_{i=0}^p|\\beta_i|\n",
    "$$\n",
    "\n",
    "Where the regularizing constant is given as C>0. This is a hyperparameter we must tune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with this model, however, is that it does not know that only 15 (10 for year $\\leq$ 1988) are selected for All-NBA. So for our predictions, we will take the top 6 players in the G category, top 6 in F, and top 3 in C (4, 4, 2 for year $\\leq$ 1988) ranking them by their outputted probabilities from the model. For cross validation (for tuning C), we will use the F1 score of these modified predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Logistic Model to NBA Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select the best C based off of k-fold cross validation, with k=5. We will also tune the class-weights as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom scoring function for CV\n",
    "from sklearn.metrics import f1_score\n",
    "def predicted_all_nba(nba_test_df, model):\n",
    "    test_df = nba_test_df.copy()\n",
    "    test_df.loc[:,'prob_all_nba'] = model.predict_proba(test_df)[:,1]\n",
    "    years = test_df['year'].unique()\n",
    "    test_df['pred_all_nba'] = 0\n",
    "    test_df['4th Team'] = 0\n",
    "    for year in years:\n",
    "\n",
    "        if year <= 1988:\n",
    "            G_all_nba = test_df[(test_df['year']==year) & (test_df['Position']=='G')].sort_values(by='prob_all_nba', ascending=False).head(6)['Player'].tolist()\n",
    "\n",
    "            F_all_nba = test_df[(test_df['year']==year) & (test_df['Position']=='F')].sort_values(by='prob_all_nba', ascending=False).head(6)['Player'].tolist()\n",
    "            \n",
    "            C_all_nba = test_df[(test_df['year']==year) & (test_df['Position']=='C')].sort_values(by='prob_all_nba', ascending=False).head(3)['Player'].tolist()\n",
    "            \n",
    "            all_nba_players = G_all_nba[0:4] + F_all_nba[0:4] + C_all_nba[0:2]\n",
    "            all_nba_players_4 = G_all_nba[4:6] + F_all_nba[4:6] + C_all_nba[2:3]\n",
    "            test_df.loc[(test_df['year']==year) & (test_df['Player'].isin(all_nba_players)), 'pred_all_nba'] = 1\n",
    "            test_df.loc[(test_df['year']==year) & (test_df['Player'].isin(all_nba_players_4)), '4th Team'] = 1\n",
    "        else:\n",
    "            G_all_nba = test_df[(test_df['year']==year) & (test_df['Position']=='G')].sort_values(by='prob_all_nba', ascending=False).head(8)['Player'].tolist()\n",
    "\n",
    "            F_all_nba = test_df[(test_df['year']==year) & (test_df['Position']=='F')].sort_values(by='prob_all_nba', ascending=False).head(8)['Player'].tolist()\n",
    "            \n",
    "            C_all_nba = test_df[(test_df['year']==year) & (test_df['Position']=='C')].sort_values(by='prob_all_nba', ascending=False).head(4)['Player'].tolist()\n",
    "            \n",
    "            all_nba_players = G_all_nba[0:6] + F_all_nba[0:6] + C_all_nba[0:3]\n",
    "            all_nba_players_4 = G_all_nba[6:8] + F_all_nba[6:8] + C_all_nba[3:4]\n",
    "\n",
    "            test_df.loc[(test_df['year']==year) & (test_df['Player'].isin(all_nba_players)), 'pred_all_nba'] = 1\n",
    "            test_df.loc[(test_df['year']==year) & (test_df['Player'].isin(all_nba_players_4)), '4th Team'] = 1\n",
    "\n",
    "    return test_df[['Player', 'year','Position', 'all_nba_tm','prob_all_nba','pred_all_nba','all_nba_c_year','4th Team']]\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def all_nba_f1(model, X, y):\n",
    "    y_pred = predicted_all_nba(X, model)['pred_all_nba']\n",
    "    return f1_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#removing linearly dependent features\n",
    "#FG = 2P+3P\n",
    "#FGA = 2PA+3PA\n",
    "#TRB = ORB+DRB\n",
    "#WS = OWS+DWS\n",
    "#BPM = OBPM+DBPM\n",
    "num_features = ['Age','G', \n",
    "                'GS', 'MP',\n",
    "                'FG', 'FGA', \n",
    "                'FG%', \n",
    "                '3P', '3PA', '3P%',\n",
    "                '2P', '2PA', \n",
    "                '2P%', 'eFG%', \n",
    "                'FT', 'FTA', 'FT%', \n",
    "                'ORB', \n",
    "                'DRB', \n",
    "                'TRB',\n",
    "                'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'PER', \n",
    "                'TS%', '3PAr', 'FTr',\n",
    "                'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', \n",
    "                'OWS',\n",
    "                'DWS',\n",
    "                'WS', \n",
    "                'WS/48', \n",
    "                'OBPM', \n",
    "                'DBPM', \n",
    "                'BPM', \n",
    "                'VORP', 'W',\n",
    "                'num_all_nba',\n",
    "                'seed'\n",
    "                ]\n",
    "\n",
    "#cat_features = ['Tm']\n",
    "\n",
    "#Now I will create a pipeline where I extract my_features, and apply OHE to cat_features\n",
    "ct = ColumnTransformer(\n",
    "    [(\"select\", \"passthrough\", num_features),\n",
    "     #(\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "     ],\n",
    "     remainder=\"drop\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "clf1 = Pipeline([\n",
    "    (\"col_transform\", ct),\n",
    "    (\"classifier\", LogisticRegression(penalty = 'l1', solver = 'liblinear', \n",
    "                                      max_iter = 10000, random_state=0\n",
    "                                      ))\n",
    "])\n",
    "\n",
    "#will create parameter grid for gridsearch for C and class_weight\n",
    "#Setting the range for class weights\n",
    "log_weights = np.linspace(0,1,30)\n",
    "\n",
    "c_grid = [0.001, 0.01, 0.1, .2, .3, .4, .5, .7, .8, 1, 3, 5 ,10, 20, 50, 100, 1000]\n",
    "\n",
    "log_param_grid = {\n",
    "    'classifier__C': c_grid,\n",
    "    'classifier__class_weight': [{0: x, 1: 1.0-x} for x in log_weights]\n",
    "}\n",
    "\n",
    "#do gridsearch using all_nba_f1 as scoring metric\n",
    "log_model = GridSearchCV(clf1, log_param_grid, cv=5,              \n",
    "                           scoring=all_nba_f1,\n",
    "                           n_jobs=-1)\n",
    "log_model.fit(nba_filt_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the following metrics for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds = predicted_all_nba(nba_filt_test, log_model)['pred_all_nba']\n",
    "log_all_preds = nba_filt_test.copy()\n",
    "log_all_preds['pred_all_nba'] = log_preds\n",
    "log_all_preds.to_csv(\"test_preds/log_preds_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, log_preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our precision and recall are not too bad for this model. Precision is the percentage of predicted positives that were true positives (how good are your guesses), and recall is the the percentage of predicted positives from the truth positives (how many of the positives did you correctly identify)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrix for these results we see that we had around 20 false postives, and false negatives. These are not ideal, but considering the size of our dataset, may still work for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test,  log_preds, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the total coefs and the nonzero coefs also see that our L1 penalty greatly reduced the numbe of features in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will extract the feature names from the pipeline\n",
    "feature_names = log_model.best_estimator_.named_steps['col_transform'].get_feature_names_out()\n",
    "coef_df = pd.DataFrame({'coef':log_model.best_estimator_['classifier'].coef_[0]\n",
    "                        ,'var':feature_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df_nz = coef_df[coef_df['coef']!=0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from our coefficient plot that FGA and wins seems to have the highest impact on the log-odds of winning All-NBA, while FT% has the biggest negative impact. This bears further study, but for the purposes of this project (prediction), we do not necessarily care about how the model is weighing each of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will make a bar chart of these coefficients\n",
    "alt.Chart(coef_df_nz).mark_bar().encode(\n",
    "    y=alt.Y('coef',title='Coefficient'),\n",
    "    x=alt.X('var',title='Variable', sort = '-y'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for this dataset we have the following players who were predicted All-NBA but did not win it. (False Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_filt_test[(log_preds==1) & (y_test!=1)][['Player',\"year\"]].sort_values(by='year', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our False Negatives we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_filt_test[(log_preds!=1) & (y_test==1)][['Player',\"year\"]].sort_values(by='year', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine what the 2023 predicted All_NBA team looks like versus the actual team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_all_nba_df(nba_test, year, model):\n",
    "    if year<=1988:\n",
    "        return 'year must be above 1988'\n",
    "    pred_df = predicted_all_nba(nba_test, model)\n",
    "    pred_df = pred_df[pred_df['year']==year]\n",
    "    pred_4 = pred_df[pred_df['4th Team']==1]\n",
    "    pred_df = pred_df[pred_df['pred_all_nba']==1]\n",
    "    pred_df.sort_values(by=['Position','prob_all_nba'], ascending=False, inplace=True)\n",
    "    pred_4.sort_values(by='Position', ascending=False, inplace=True)\n",
    "    pred_team1 = pred_df.iloc[[0,1,6,7,12],:]['Player'].tolist()\n",
    "    pred_team2 = pred_df.iloc[[2,3,8,9,13],:]['Player'].tolist()\n",
    "    pred_team3 = pred_df.iloc[[4,5,10,11,14],:]['Player'].tolist()\n",
    "    pred_team4 = pred_4['Player'].tolist()\n",
    "    all_nba_df = pd.DataFrame({'Predicted Team 1':pred_team1, 'Predicted Team 2':pred_team2, 'Predicted Team 3':pred_team3, 'Predicted Team 4':pred_team4})\n",
    "    all_nba_df.index = ['Guard', 'Guard', 'Forward', 'Forward', 'Center']\n",
    "    return all_nba_df.transpose()\n",
    "\n",
    "def true_all_nba(nba_test,year):\n",
    "    teams = ['1st','2nd','3rd']\n",
    "    if year<=1988:\n",
    "        return 'year must be above 1988'\n",
    "    year_all_nba = nba_test[(nba_test['year']==year) & (nba_test['all_nba_tm'].isin(teams))].sort_values(by=['all_nba_tm','Position'], ascending=True)\n",
    "    team1 = year_all_nba.iloc[np.arange(0,5),:]['Player'].tolist()\n",
    "    team2 = year_all_nba.iloc[np.arange(5,10),:]['Player'].tolist()\n",
    "    team3 = year_all_nba.iloc[np.arange(10,15),:]['Player'].tolist()\n",
    "    all_nba_df = pd.DataFrame({'True Team 1':team1, 'True Team 2':team2, 'True Team 3':team3})\n",
    "    all_nba_df.index = ['Center', 'Forward', 'Forward', 'Guard', 'Guard']\n",
    "   \n",
    "    return  all_nba_df.iloc[::-1].transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_all_nba(nba_filt_test, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all_nba_df(nba_filt_test, 2023, log_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is considered an \"Ensemble\" Model. These types of models take in a large number of different models that are performing slightly better than chance, and then combine them into a stronger model. For a random forest, this means that `n-estimator` total decision trees are created, then their results are combined (by  picking the result the majority of trees agree with). Each tree is trained on random subsets of the data, with random subsets of the features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also begs the question, why do these models work? We can consider a simple example, where we can imagine that we have a `n` total models that predict the correct results, say 51% of the time. If we assume each model is independent in their predictions, then we can utilize a bernoulli model and apply the law of large numbers. In simpler terms, we can imagine every model is like a coin flip, where the true result is a heads (with 51% chance), and the incorrect result is tails (49%). If we consider an ensemble of 10 models, then we will simply consider how many heads we have vs tails. If we have more heads, we pick that result, and more tails we pick the other result. Clearly, this will lead to quite a bit of variance in our results, and we will often pick the wrong result. But imagine we are flipping 1000 coins, or even 100000 coins. We see that we will have, with probability approaching 1, that as our number of models grow, the number of heads will be greater than the number of tails. In our case, the independence assumption is violated since we utilize the same data, however, we can still yield good results even without this assumption. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Random Forest Model to NBA Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit this model we will consider a grid of hyper-parameters. Since this grid is quite large (7200), we will utilize a randomized search where we consider 500 random subsets of the hyper-parameters.\n",
    "\n",
    "We can also consider what these hyper-parameters mean.\n",
    "1) n_estimators: How many decision trees will be created in total\n",
    "2) max_depth: The longest path between the root node and the leaf. This is essentially controlling how many splits are allowed.\n",
    "3) min_samples_lead: Minimum number of samples to required for a leaf node. This determines how specific a leaf can be.\n",
    "4) min_samples_split: Minimum number of samples a node must have for it to be split\n",
    "5) max_features: Maximum number features randomly selected for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Now we will fit a random forest model to the data. We will fit this data into a pipeline to scale the data and then fit the model.\n",
    "\n",
    "num_features = ['Age','G', \n",
    "                'GS', 'MP',\n",
    "                'FG', 'FGA', \n",
    "                'FG%', \n",
    "                '3P', '3PA', '3P%',\n",
    "                '2P', '2PA', \n",
    "                '2P%', 'eFG%', \n",
    "                'FT', 'FTA', 'FT%', \n",
    "                'ORB', \n",
    "                'DRB', \n",
    "                'TRB',\n",
    "                'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'PER', \n",
    "                'TS%', '3PAr', 'FTr',\n",
    "                'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', \n",
    "                'OWS',\n",
    "                'DWS',\n",
    "                'WS', \n",
    "                'WS/48', \n",
    "                'OBPM', \n",
    "                'DBPM', \n",
    "                'BPM', \n",
    "                'VORP', 'W',\n",
    "                'num_all_nba',\n",
    "                'seed'\n",
    "                ]\n",
    "\n",
    "#cat_features = ['Tm']\n",
    "\n",
    "#Now I will create a pipeline where I extract my_features, and apply OHE to cat_features\n",
    "ct2 = ColumnTransformer(\n",
    "    [(\"select\", \"passthrough\", num_features),\n",
    "     #(\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "     ],\n",
    "     remainder=\"drop\"\n",
    ")\n",
    "\n",
    "clf2 = Pipeline([('col_transform',ct2),\n",
    "                ('classifier',RandomForestClassifier(random_state=0))\n",
    "                ])\n",
    "\n",
    "# rf_model = Pipeline([('col_transform',ct),\n",
    "#                 ('classifier',RandomForestClassifier(random_state=0,\n",
    "#                                                      n_estimators=200,\n",
    "#                                                      min_samples_split=2,\n",
    "#                                                      min_samples_leaf=5,\n",
    "#                                                      max_features=.2,\n",
    "#                                                      max_depth=20,\n",
    "#                                                      class_weight='balanced_subsample'))\n",
    "#                 ])\n",
    "# rf_model.fit(nba_filt_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can fit our Random Forest model to the data.\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_param_grid = {\n",
    "   'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "   'classifier__max_depth': [5, 10, 15, 20, 25],\n",
    "   'classifier__min_samples_split': [2, 5, 10, 15, 20, 25, 30, 40],\n",
    "   'classifier__min_samples_leaf': [5, 10, 15, 20],\n",
    "   'classifier__max_features': [ 'sqrt', 'log2', 0.2, .4],\n",
    "   'classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "}\n",
    "\n",
    "\n",
    "rf_model = RandomizedSearchCV(estimator=clf2, param_distributions=rf_param_grid, cv= 5,\n",
    "                            random_state=0, n_iter = 3000,\n",
    "                            scoring=all_nba_f1,\n",
    "                            n_jobs=-1)\n",
    "rf_model.fit(nba_filt_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may look at how our model did on our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds = predicted_all_nba(nba_filt_test, rf_model)['pred_all_nba']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have for our precision, recall, and f1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rf_preds, target_names=['Not All-NBA', 'All-NBA']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our confusion matrix we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, rf_preds, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view our false positives as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False Positives\n",
    "nba_filt_test[(rf_preds==1) & (y_test==0)][['Player','year']].sort_values(by='year', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our False negatives we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False Negatives\n",
    "nba_filt_test[(rf_preds==0) & (y_test==1)][['Player','year']].sort_values(by='year', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = rf_model.best_estimator_.named_steps['col_transform'].get_feature_names_out()\n",
    "coef_df = pd.DataFrame({'coefs':rf_model.best_estimator_['classifier'].feature_importances_,\n",
    "                          'features':feature_names})\n",
    "coef_df_nz = coef_df[coef_df['coefs']!=0]\n",
    "coef_df_nz.sort_values(by='coefs', ascending=False, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view which coefficients made the most impact on the model. We see similar to our logistic regression, VORP was one of the strongest features in prediciting All-NBA status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(coef_df_nz[0:40]).mark_bar().encode(\n",
    "    y='coefs',\n",
    "    x=alt.Y('features', sort='-y'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine what the 2023 predicted All_NBA team looks like versus the actual team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_all_nba(nba_filt_test, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all_nba_df(nba_filt_test, 2023, rf_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost is a more recent classification algorithm that also makes use of ensemble learners. In this algorithm, we similarly create decision trees, but rather than each tree being independent, each successive tree tries to improve on its predecessor. This is done by trying to fit to residual values, and find any underlying patterns that may be there. The algorithm is stopped when the residuals are sufficiently random and no more patterns can be found. Similar to Random Forests, we have quite a bit of hyper parameters to tune, so a randomized CV search is utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#Now we will fit a random forest model to the data. We will fit this data into a pipeline to scale the data and then fit the model.\n",
    "\n",
    "num_features = ['Age','G', \n",
    "                'GS', 'MP',\n",
    "                'FG', 'FGA', \n",
    "                'FG%', \n",
    "                '3P', '3PA', '3P%',\n",
    "                '2P', '2PA', \n",
    "                '2P%', 'eFG%', \n",
    "                'FT', 'FTA', 'FT%', \n",
    "                'ORB', \n",
    "                'DRB', \n",
    "                'TRB',\n",
    "                'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'PER', \n",
    "                'TS%', '3PAr', 'FTr',\n",
    "                'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', \n",
    "                'OWS',\n",
    "                'DWS',\n",
    "                'WS', \n",
    "                'WS/48', \n",
    "                'OBPM', \n",
    "                'DBPM', \n",
    "                'BPM', \n",
    "                'VORP', 'W',\n",
    "                'num_all_nba',\n",
    "                'seed'\n",
    "                ]\n",
    "\n",
    "#cat_features = ['Tm']\n",
    "\n",
    "#Now I will create a pipeline where I extract my_features, and apply OHE to cat_features\n",
    "ct3 = ColumnTransformer(\n",
    "    [(\"select\", \"passthrough\", num_features),\n",
    "     #(\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "     ],\n",
    "     remainder=\"drop\"\n",
    ")\n",
    "\n",
    "scale_pos = (len(y_train) - sum(y_train))/sum(y_train)\n",
    "\n",
    "clf3 = Pipeline([('col_transform',ct3),\n",
    "                     ('classifier',xgb.XGBClassifier(random_state=0, scale_pos_weight = scale_pos))\n",
    "                     ])\n",
    "xgb_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'classifier__max_depth': [5, 10, 15, 20, 25],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, .3],\n",
    "    'classifier__min_child_weight': [1, 2, 5, 10, 15],\n",
    "    'classifier__gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'classifier__colsample_bytree': [0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    'classifier__subsample': [0.3, 0.4, 0.5, 0.6, 0.7,1]\n",
    "}\n",
    "\n",
    "xgb_param_grid_test = {\n",
    "    'classifier__n_estimators': [100]\n",
    "}\n",
    "\n",
    "\n",
    "xgb_model = RandomizedSearchCV(estimator=clf3, param_distributions=xgb_param_grid, cv= 5, n_iter = 3000,\n",
    "                           random_state=0,\n",
    "                           scoring=all_nba_f1,\n",
    "                           n_jobs = -1\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(nba_filt_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds = predicted_all_nba(nba_filt_test, xgb_model)['pred_all_nba']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our metrics for this model as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, xgb_preds, target_names=['Not All-NBA', 'All-NBA']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, xgb_preds, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our feature importance, we see that the advanced statistic PER is the most impactful, with VORP being the second most so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = xgb_model.best_estimator_.named_steps['col_transform'].get_feature_names_out()\n",
    "coef_df = pd.DataFrame({'coefs':xgb_model.best_estimator_['classifier'].feature_importances_,\n",
    "                          'features':feature_names})\n",
    "coef_df_nz = coef_df[coef_df['coefs']!=0]\n",
    "\n",
    "alt.Chart(coef_df_nz).mark_bar().encode(\n",
    "    y='coefs',\n",
    "    x=alt.Y('features', sort='-y'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our false positives we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_filt_test[(xgb_preds==1) & (y_test!=1)][['Player',\"year\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our false negatives we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_filt_test[(xgb_preds!=1) & (y_test==1)][['Player',\"year\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine what the 2023 predicted All_NBA team looks like versus the actual team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_all_nba(nba_filt_test, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all_nba_df(nba_filt_test, 2023, xgb_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to use an ensemble learner using all 3 methods utilized so far: Logistic Regression, Random Forests, and XGboost. We will use soft voting (averaging over all the predicted probabilities) to classify the players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemb_clf = VotingClassifier(estimators=[('lr', log_model.best_estimator_), ('rf', rf_model.best_estimator_), ('xgb', xgb_model.best_estimator_)],\n",
    "                              voting='soft', weights=[1,1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemb_clf.fit(nba_filt_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following metrics for this ensemble learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_preds = predicted_all_nba(nba_filt_test, ensemb_clf)['pred_all_nba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, ens_preds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we end up with similar precision, recall as our logistic regression, indicating we did not do that much better than the individual models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our confusion matrix we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, ens_preds, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final metric we can also view the ROC curve. This plots the true positive rate vs the false positive rate. To measure how effective our classifier is, we consider area under the curve (AUC). A perfect classifier would have an AUC of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can draw the ROC curves for all 3 models and the ensemble model.\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "#Now we may plot them,\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(*roc_curve(y_test, log_model.predict_proba(nba_filt_test)[:,1])[:2], label=\"Logistic Regression (area = %0.3f)\" % auc(*roc_curve(y_test, log_model.predict_proba(nba_filt_test)[:,1])[:2]))\n",
    "plt.plot(*roc_curve(y_test, rf_model.predict_proba(nba_filt_test)[:,1])[:2], label=\"Random Forest (area = %0.3f)\" % auc(*roc_curve(y_test, rf_model.predict_proba(nba_filt_test)[:,1])[:2]))\n",
    "plt.plot(*roc_curve(y_test, xgb_model.predict_proba(nba_filt_test)[:,1])[:2], label=\"XGBoost (area = %0.3f)\" % auc(*roc_curve(y_test, xgb_model.predict_proba(nba_filt_test)[:,1])[:2]))\n",
    "plt.plot(*roc_curve(y_test, ensemb_clf.predict_proba(nba_filt_test)[:,1])[:2], label=\"Ensemble(area = %0.3f)\" % auc(*roc_curve(y_test, ensemb_clf.predict_proba(nba_filt_test)[:,1])[:2]))\n",
    "plt.legend()\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "\n",
    "auc_scores = [auc(*roc_curve(y_test, log_model.predict_proba(nba_filt_test)[:,1])[:2]),\n",
    "                auc(*roc_curve(y_test, rf_model.predict_proba(nba_filt_test)[:,1])[:2]),\n",
    "                auc(*roc_curve(y_test, xgb_model.predict_proba(nba_filt_test)[:,1])[:2]),\n",
    "                auc(*roc_curve(y_test, ensemb_clf.predict_proba(nba_filt_test)[:,1])[:2])]\n",
    "              \n",
    "print(\"Logistic Regression AUC: \", auc_scores[0])\n",
    "print(\"Random Forest AUC: \", auc_scores[1])\n",
    "print(\"XGBoost AUC: \", auc_scores[2])\n",
    "print(\"Ensemble AUC: \", auc_scores[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine what the 2023 predicted All_NBA team looks like versus the actual team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_all_nba(nba_filt_test, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all_nba_df(nba_filt_test, 2023, ensemb_clf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may put all our results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "#Putting precision and recall and ROC-AUC score into a dataframe for all 4 models\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'XGBoost', 'Ensemble']\n",
    "precision = [0,0,0,0]\n",
    "recall = [0,0,0,0]\n",
    "roc_auc = [0,0,0,0]\n",
    "f1_scores = [0,0,0,0]\n",
    "accuracy = [0,0,0,0]\n",
    "for i, preds in enumerate([log_preds, rf_preds, xgb_preds, ens_preds]):\n",
    "    precision[i] = precision_score(y_test, preds)\n",
    "    recall[i] = recall_score(y_test, preds)\n",
    "    roc_auc[i] = auc_scores[i]\n",
    "    f1_scores[i] = f1_score(y_test, preds)\n",
    "    accuracy[i] = accuracy_score(y_test, preds)\n",
    "res_df = pd.DataFrame({'Model': model_names, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F-1 Score':f1_scores ,'ROC-AUC': roc_auc})\n",
    "res_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
